<!DOCTYPE html><html lang="en"><head><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-160213843-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160213843-1');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, shrink-to-fit=no"><meta name="keywords" content="Computational Photography,Deep Sensing,Deep Optics,Task-Oriented Sensing,Video Compressive Sensing,Coded Exposure Camera,Coded Aperture Camera,Light Field Camera,Focus Sweep Camera,Time of Flight,Depth From Defocus,Multi-tap CMOS Image Sensor,コンピュテーショナルフォトグラフィ,符号化露光カメラ,符号化露光画像,符号化開口カメラ,ライトフィールドカメラ,圧縮ビデオセンシング,マルチタップCMOSイメージセンサ,フォーカススイープカメラ"><title>KAKENHI(S) - Nagahara Lab.</title><meta name="description" content="長原研究室ではコンピュテーショナルフォトグラフィ (Computational Photography) をテーマに研究している． 従来のカメラは人間が見て意味のあるものを撮像するが，コンピュータビジョンの様々なタスクで使用する際にはカメラは必ずしも人間が見て意味のあるものを撮像する必要はない． コンピュテーショナルフォトグラフィでは，従来の撮像過程を改変し後のタスクに適したセンシングを考えることで従来なし得なかった撮像・画像処理を可能にする． 我々の研究室ではこれまでに符号化露光カメラや符号化開口カメラ，フォーカススイープカメラなどの様々なタスクに応用可能なカメラを開発・研究してきた．"><link rel="shortcut icon" type="image/png" href="/favicon.ico"><link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" type="text/css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" type="text/css" rel="stylesheet"><link href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" rel="stylesheet" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" type="text/css" rel="stylesheet"><link href="/assets/main.css" type="text/css" rel="stylesheet"><link href="/feed.xml" type="application/rss+xml" title="Nagahara Lab." rel="alternate"><link href="https://nagahara-lab.github.io/projects/kibanS-17H06102/" rel="canonical"></head><body><nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav"><div class="container"><a class="navbar-brand" href="/">Nagahara Lab.</a><button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><Menu></Menu><i class="fa fa-bars"></i></button><div class="collapse navbar-collapse" id="navbarResponsive"><ul class="navbar-nav ml-auto"><li class="nav-item"><a class="nav-link" href="/">Home</a></li><li class="nav-item"><a class="nav-link" href="/projects">Projects</a></li><li class="nav-item"><a class="nav-link" href="/members">Members</a></li><li class="nav-item"><a class="nav-link" href="/publications">Publications</a></li><li class="nav-item"><a class="nav-link" href="/news">News</a></li><li class="nav-item"><a class="nav-link" href="/contact">Contact</a></li></ul></div></div></nav><header class="masthead" style="background-image: url('/assets/imgs/projects/kibanS-17H06102/fig2.webp')"><div class="overlay"></div><div class="container"><div class="row"><div class="col-lg-8 col-md-10 mx-auto"><div class="page-heading"><h1>KAKENHI(S)</h1><span class="subheading">多元コンピュテーショナル光計測による手術支援応用 - Computational Optical Imaging for Endoscopic Surgery</span></div></div></div></div></header><div class="container"><div class="row"><div class="col-lg-8 col-md-10 mx-auto"><h2>多元コンピュテーショナル光計測による手術支援応用 - Computational Optical Imaging for Endoscopic Surgery</h2><ul><li>課題番号：17H06102</li><li>研究代表者:<ul><li><a href="/members/nagahara/"><b>長原 一（NAGAHARA, HAJIME）</b></a></li><li>大阪大学・データビリティフロンティア機構・教授</li></ul></li></ul><br/><h3><a name="abst">研究の概要</a></h3><p style="text-indent:1em">近年，開腹手術とくらべて患者の負担が少なく快復が早いことから内視鏡手術が注目されている．しかし，内視鏡で得られるのは視野の狭い2次元画像で，術者に高度な技術を要求することから開腹手術と比べて効率や安全性が劣るという問題がある．本研究では，新たな光計測技術を開発し非接触でリアルタイムの臓器の3次元計測･推定手法を実現する．</p><ul><li>研究分野：情報学，知覚情報処理</li><li>キーワード：コンピュテーショナルフォトグラフィ，光センシング，医療計測</li></ul><br/><h3><a name="sec1">１．研究開始当初の背景</a></h3><p style="text-indent:1em">近年，開腹手術とくらべて患者の負担が少なく快復が早いことから内視鏡手術が注目され，適用数は増加の一途をたどっている．しかし，一般的な内視鏡で得られる情報はモニタに表示される視野の狭い2次元画像のみで腹腔へ挿入しているため視点の自由も限定的である．そのため術者に高度な技術を要求することから開腹手術と比べて効率や安全性が劣るという問題がある．内視鏡下による臓器の3次元リアルタイム計測が可能となれば，これまで術者の手探りや勘に頼っていた職人的な術式から，定量化･客観化された情報に基づく効率的で安全な手術の実現が期待できる．</p><p style="text-indent:1em">工業計測などで用いられるTime of flight (TOF)による距離計測は，正弦波でモジュレーションされた光を物体に投影し，物体上で反射した光をセンサで計測する．その反射光の位相を自己相関により求め，位相差から求まる光の到達時間の遅れから物体の距離や形状を計測する手法である．従来のTOFでは，<a href='#fig1'>図１</a>に示す直接反射のみを想定しているが，実際の反射光は物体内での散乱成分や他の物体からの間接反射によるマルチパス成分を含んでる．これらの乱反射による複数の正弦波の合成波も正弦波であることから直接反射とその他の反射の成分が分離できず，推定距離や形状に大きな誤差を生じる．そのため，臓器などの生体組織は，複雑な反射を起こすことから，これまでTOFによる形状計測は適用できなかった．<a name="fig1"><img src="/assets/imgs/projects/kibanS-17H06102/fig1.png" alt="図1" width="100%"/></a><span>図１：Time of Flight (TOF)計測での反射光の応答</span></p><br/><h3><a name="sec2">２．研究の目的</a></h3><p style="text-indent:1em">本研究では，新たな光計測技術を開発し非接触でリアルタイムの臓器の3次元計測･推定手法を実現する．この手法を内視鏡手術に応用することで，これまで術者の手探りや勘に頼っていた職人的な術式から，定量化･客観化された情報に基づいた効率的で安全な手術の実現が期待できる．実際に，この3次元計測技術用いた内視鏡手術支援システムを構築して，動物実験や臨床により提案手法の有効性を実証する．</p><h3><a name="sec3">３．研究の方法</a></h3><p style="text-indent:1em">本研究では，投影光源や撮像センサの開発による新たな符号化･復調化により反射光から直接反射，散乱成分を抽出することができる光コム干渉カメラを提案する．このカメラで得られる干渉画像から臓器の形状を推定する手法を提案し，医療応用を対象とした実証を行う．</p><p style="text-indent:1em"><a href="#fig2">図２</a>に研究提案の概要と要素について示す．光コム干渉カメラは，光周波数コム光源と光干渉光学系，時間変調CMOSセンサを備え，光コム光源を投影光として物体に照射し，物体からの反射光を捉える．反射光は，ビームスプリッタを経由してカメラ内部に備わる参照光源から発せられる光と干渉することで，センサ上に光干渉画像を生じる．この干渉画像を新規開発する時間変調CMOSセンサによりデジタルデータとして計測する．単一の計測手段により得られた画像から，異なる反射光の情報を取りだし，臓器の表面や表層，深層の形状をそれぞれTOF, OCT, DFD/DOTといった異なる推定手法で計測し，レンジや特性の異なるそれら推定結果を医療応用が求めるシームレスな統合モデルとして融合することが本研究の特徴である．<a name="fig2"><img src="/assets/imgs/projects/kibanS-17H06102/fig2.png" alt="図2" width="100%"/></a><span>図２：研究概要と要素</span></p><br/><h3><a name="sec4">４．これまでの成果</a></h3><p style="text-indent:1em">光コム干渉カメラの要素技術である，TOFとOCT計測を同時に実現する光学システムを構築した(<a href="#fig3">図３</a>)．また，その計測画像を撮像するコンピュテーショナル超高速CMOSイメージセンサの１号試作を行った(<a href="#fig4">図４</a>)．高周波照明を用いることで，物体内部の直接反射光と間接反射光を分離することで，散乱やマルチパスを起こす生体組織のような半透明の物体の正確な形状計測が可能な手法を実現した(<a href="#fig5">図５</a>)．また，画像より生体組織の反射吸収係数を求める方法を提案した．臓器のような弾性変形のある物体に内部構造などの重畳表示を行うための弾性変形に対応した物体表面形状のトラッキング手法を開発した．さらには，臨床応用の足掛かりとして，内視鏡画像から臓器の酸素飽和度を可視化して重畳する内視鏡システムの実装と検証を始めた．<div class="row"><div class="col-lg-6 col-md-6 mx-auto"><a name="fig3"><img src="/assets/imgs/projects/kibanS-17H06102/fig3.png" alt="図3" width="100%"/></a><span>図３: TOF/OCTの統合システム</span></div><div class="col-lg-6 col-md-6 mx-auto"><a name="fig4"><img src="/assets/imgs/projects/kibanS-17H06102/fig4.png" alt="図4" width="100%"/></a><span>図４: 試作した超高速CMOSイメージセンサ</span></div></div><br/><div class="row"><div class="col-lg-12 col-md-12 mx-auto"><a name="fig5"><img src="/assets/imgs/projects/kibanS-17H06102/fig5.png" alt="図5" width="50%"/></a><br/><span>図５：高周波パターン投影によるToFシステム<br/>上段：システムの外観、下段左：通常のTOF(直方体計測)、下段右：提案手法</span></div></div></p><br/><h3><a name="sec5">５．これまでの発表論文等（受賞等も含む）</a></h3><ol><li>I. Nishidate, M. Minakawa, D. Mcduff, Md. Abdul Wares, K. Nakano, H. Haneishi, Y. Aizu, and K. Niizeki, “Simple and affordable imaging of multiple physiological parameters with RGB camera-based diffuse reflectance spectroscopy,” Biomedical Optics Express, 11(2), pp.1073-1091 (2020).</li><li>Michitaka Yoshida , Toshiki Sonoda, Hajime Nagahara, Kenta Endo, Yukinobu Sugiyama, and Rin-ichiro Taniguchi, “High-Speed Imaging Using CMOS Image Sensor With Quasi Pixel-Wise Exposure”, IEEE TRANSACTIONS ON COMPUTATIONAL IMAGING, Vol. 6, pp. 463-476 (2020).</li><li>Futa Mochizuki, Keiichiro Kagawa, Ryota Miyagi, Min-Woong Seo, Bo Zhang, Taishi Takasawa, Keita Yasutomi, Shoji Kawahito, “Separation of multi-path components in sweep-less time-of-flight depth imaging with a temporally-compressive multi-aperture image sensor,” ITE Trans. on MTA, Vol. 6, Issue 3, pp. 202-211 (Jul., 2018).</li><li>Takuya Yoda, Hajime Nagahara, Rin-ichiro Taniguchi, Keiichiro Kagawa, Keita Yasutomi and Shoji Kawahito, “The Dynamic Photometric Stereo Method Using a Multi-Tap CMOS Image Sensor”, MDPI Sensors, Vol. 18, No. 3 (Mar., 2018).</li><li>K. Ota and Y. Hayasaki, “Complex-amplitude single-pixel imaging,” Opt. Lett. 43, 3682-3685 (2018).</li></ol><br/></div></div></div><hr><footer><div class="container"><div class="row"><div class="col-lg-8 col-md-10 mx-auto"><ul class="list-inline text-center"><li class="list-inline-item"><a href="mailto:contact@ids.osaka-u.ac.jp"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i><i class="far fa-envelope fa-stack-1x fa-inverse"></i></span></a></li>



<li class="list-inline-item"><a href="https://github.com/nagahara-lab"><span class="fa-stack fa-lg"><i class="fas fa-circle fa-stack-2x"></i><i class="fab fa-github fa-stack-1x fa-inverse"></i></span></a></li></ul><br><p style="font-size: 1.5rem;">Osaka University Institute for Datability Science</p><p style="font-size: 1rem;">TechnoAlliance Building C503, 2-8 Yamadaoka, Suita, Osaka 565-0871 Japan</p><p class="copyright text-muted">Copyright &copy; Nagahara Lab. 2020</p></div></div></div></footer><script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/startbootstrap-clean-blog/5.0.8/css/clean-blog.min.css"></script>

<script src="/assets/scripts.js"></script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script>
$(function (){
  $(".language-bibtex").each(function(){
    var $this = $(this);
    var title = $this.text().match(/[^kK]title\s?=\s?{(.*?)},?/)[1] || "BibTeX";
    $this.hide();
    $this.parent().before($("<a>", {text: title, "data-status": "false", "class": "bibtex"}).click(function(){
      if($(this).data("status")) {
        $(this).data("status", false);
        $this.hide();
      } else {
        $(this).data("status", true);
        $this.show();
      }
    }));
  });
});
</script>



</body></html>